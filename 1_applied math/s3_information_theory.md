# 第三章 情報理論 

<br>

## 目的
### 1) 自己情報量・シャノンエントロピーの定義を確認する

### 2) KLダイバージェンス・交差エントロピーの概要を知る

<br>

# レポート

logを使っていると情報量を表している、という考え方もある

## 自己情報量

- logの底数が 2 なので単位はbit、と覚えておく
- 自己情報量が「確率の関数」・「減少関数」・「加法性」の３つの性質を持って下記で表現した関数がちょうど３つの性質を満たすため、このように定義した

<br>

![s3_jikojoho.jpg](img/s3_jikojoho.jpg)

<br>

## シャノン・エントロピー

- 自己情報量の期待値
- 値が最大になる変数を探す

<br>

![s3_syanon.jpg](img/s3_syanon.jpg)

<br>

## カルバック・ライブラー（KL）ダイバージェンス

- 同じ事象・確率変数における異なる確率分布P(x)、Q(x)の違いを表す

<br>

![s3_kld.jpg](img/s3_kld.jpg)

<br>

## 交差エントロピー

- Q(x)についての自己情報量をP(x)の分布で平均したもの

<br>

![s3_kousae.jpg](img/s3_kousae.jpg)

<br>


---

# 気づき
- P(x)、Q(x)の確率が同じ値であるとき、KLダイバージェンス値は0になる<br>

