# 深層学習Day3 第五章 Seq2Seq

## Seq2Seq
- Encoder-Decoderモデルの一種を指す

## Seq2seqの具体的な用途
- 機械対話や、機械翻訳などに使用されている

## Encoder RNN
- ユーザーがインプットしたテキストデータを、単語等のトークンに区切って渡す構造
- Taking :文章を単語等のトークン毎に分割し、トークンごとのIDに分割する。-
- Embedding :IDから、そのトークンを表す分散表現ベクトルに変換。
- Encoder RNN:ベクトルを順番にRNNに入力していく

## Decoder RNN
- システムがアウトプットデータを、単語等のトークンごとに生成する構造

## 確認テスト1
- 選択肢から、seq2seqについて説明しているものを選べ
- （2）RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる
- 考察
    - (1)は双方向RNN、(3)は単純RNN、(4)はLSTM

## 演習チャレンジ1
- （問題文は省略）
- （1）E.dot(w)
- 単語wはone-hotベクトルであり、それを単語埋め込みにより別の特徴量に変換する。これは埋め込み行列Eを用いて、E.dot(w)と書ける
- 考察
    - またもやdot関数の出番だ。

## HRED
- 過去n-1 個の発話から次の発話を生成する
- 前の単語の流れに即して応答されるため、より人間らしい文章が生成される

## HREDの課題
- HRED は確率的な多様性が字面にしかなく、会話の「流れ」のような多様性が無い。
    - 同じコンテキスト（発話リスト）を与えられても、答えの内容が毎回会話の流れとしては同じものしか出せない。
    - HRED は短く情報量に乏しい答えをしがちである。
        - 短いよくある答えを学ぶ傾向がある。
            - ex)「うん」「そうだね」「・・・」など

## VHRED
- HREDに、VAEの潜在変数の概念を追加したもの

## 確認テスト2
- seq2seqとHRED、HREDとVHREDの違いを簡潔に述べよ
- seq2seq・・・1文の1問一答に対して処理ができるある時系列データからある時系列データを作り出すネットワーク
- HRED・・・seq2seqの思考にそれまでの文脈の意味ベクトルを解釈に加えられるようにすることで文脈の意味を考慮したEncode、Decodeができるようにしたもの
- VHRED・・・HREDが当たり障りのない回答しか作れなくなったことに対しての解決策。オートエンコーダーの考え方を取り入れて改良を施したモデル。
- 考察
    - 他のサイトでも似たような考察があったので以下に掲載
    - Seq2Seq は一問一答だが、これを過去の n-1 個の発話から次の n 個目の発話を推測するようにしたのが HRED
    - HRED は会話の「流れ」のようなロングタームな多様性が無いが、VHREDは会話の流れを表す Context RNN にノイズを乗せることで、同じコンテキストに対しても字面だけではない多様な返答ができる
    - HRED は確率的な多様性が字面にしかないが、VHRED はコンテキストに対する返答のばらつきを Context RNN の確率的な幅で吸収することでそれらをうまく学習できる
    - 後発の技術のほうが明らかに優れているように思えるが、実際に使用する場合はきちんとデメリットも把握した上で使用するようにしたい

## VAE
- オートエンコーダーと呼ばれる教師なし学習モデル

## 確認テスト3

- VAEに関する下記の説明文中の空欄に当てはまる言葉を答えよ
    - 自己符号化器の潜在変数に____を導入したもの
- 確率分布
- 考察
    - このあたりからいきなり確率分布の考え方が組み込まれて急に難しくなるので注意。

---

# ハンズオン

- 該当するハンズオンはなし

---

# 気づき
- ディープラーニングによる言語処理は画像処理と並ぶ花形分野だと思われる。実際に自分でもうまく使うことができたらいいなと思う場面は日常でたくさんあるので、しっかりと勉強して身につけておきたい。
